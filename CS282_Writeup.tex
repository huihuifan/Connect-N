\documentclass[12pt]{article}

\usepackage{fullpage}	
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}	
\usepackage[square, authoryear, sort, comma, numbers]{natbib}	
\usepackage{setspace}
\usepackage[T1]{fontenc}
\usepackage{titling}
\usepackage{enumerate}
\usepackage{float}
\usepackage{titlesec}
\usepackage{makeidx}
\usepackage{wrapfig}

\titleformat{\section}
  {\normalfont\fontsize{14}{15}\bfseries}{\thesection}{1em}{}

\titleformat{\subsection}
  {\normalfont\fontsize{14}{15}\bfseries}{\thesubsection}{1em}{}

\setlength{\droptitle}{-5em}   
\newcommand*{\TitleFont}{%
      \usefont{\encodingdefault}{\rmdefault}{n}{b}%
      \fontsize{14}{16}%
      \selectfont}

\begin{document}

\title{\TitleFont \textbf{Connect N:\\ A Reinforcement Learning Approach}}
\author{\TitleFont Lucy Cheng, Angela Fan, Andre Nguyen}
\maketitle

\vspace{-1.5cm}

\section*{ALL FIGURES NEED CAPTIONS STILL}

\section{Problem Description}

The problem we chose to work on is that of the classic children's game, Connect-4, but extrapolated to be "Connect-N", where players strive to create streaks of length N with their tokens, while stopping their opponent from creating N-length streaks.

\section{Simulator and Driver Function Descriptions}

The \textbf{simulator} takes two inputs:
\begin{enumerate}
\item \textbf{n}, the number of tokens that must be connected by a player to win
\item \textbf{grid\_size}, the height of the square board that will be created by the simulator
\end{enumerate}

The simulator initializes a nested NumPy array of zeroes, where each nested array is a row of the board. Zero represents an empty state. Players 1 and -1 can tell the simulator to make a move by calling the simulator's move method. The board state then adds a 1 or -1 (depending on the player) in the correct spot. When the move method is called, the simulator automatically checks if a player has won. 

The move function returns:

\begin{enumerate}
\item 1 if the current player making a move has won
\item -1 if the current player making a move is trying to make an illegal move (for example: column does not exist, column is already full)
\item 0 otherwise
\end{enumerate}

and a reward:
\begin{enumerate}
\item 50 if the current player making a move has won
\item 0 otherwise
\end{enumerate}

The simulator has a number of helper methods as well:
\begin{enumerate}
\item \textbf{simulate\_move}, which tests to see if a move is valid and returns 1 if not valid
\item \textbf{turn}, which returns whose turn it is 
\item \textbf{next\_possible\_moves}, which returns an array of all possible columns available for a next move (i.e. eliminates columns that are full)
\item \textbf{all\_tokens\_placed}, which returns the location as a (column, row) tuple of all of the tokens that have been placed					
\item \textbf{is\_empty}- takes in a location and returns True if the location is empty and False otherwise
\item \textbf{streak} functions that check if a given player has a horizontal, vertical, or diagonal streak going
\item \textbf{print\_grid}, which prints the current board state
\end{enumerate}

We have written a separate function, called \textbf{play\_game}, that serves as a driver function to run the Connect N game (diagrammed in \textbf{Figure 1}).  The driver function takes as input two learners, and uses the Connect N simulator to create a game. The game asks each learner for an action and takes the specified moves. If neither player has won, it continues the game by passing reward and the new board state back to the learners.

\includegraphics[scale=.7]{game_process.png}

\section{State and Action Spaces}

Possible \textbf{actions} are integers representing the numerical column number that a player wishes to put their token into.

The \textbf{states} are boards, represented as NumPy arrays (see Figure 2).

\includegraphics[scale=.7]{matrix.png} 

\section{Approaches}
\textbf{THIS IS COMBINED MOTIVATION FOR APPROACHES + DETAILED DESCRIPTION OF APPROACHES}

\includegraphics[scale=.5]{learners.png}

\subsubsection{Baselines} 

We implemented three baselines:
\begin{enumerate}
\item \textbf{Random action}, where the agent randomly chooses a column to put its token into at each time step
\item \textbf{Extend}, where the agent attempts to extend its largest current streak (horizontally, vertically, or diagonally), breaking ties between streaks randomly.
\item \textbf{Minimax}--- We implemented the Artificial Intelligence algorithm Minimax, which conceptualizes the Connect-N game as a tree of possible future game states. The current state of the game is the root node of the tree, and each of the children represent possible states resulting from a move that can be made from the current game state. The leaves are the final states of the game, or states where no further moves can be made because one player has one or lost. In traditional Minimax, leaves have a value of either positive infinity (win), negative infinity (lose), or 0 (tie), but such an algorithm forces Minimax to continue evaluating future board states until it reaches a win or a lose. However, this is often computationally infeasible as there are so many possible states in the game. Instead, we implement a \textbf{static evaluation} form of Minimax that takes a \textbf{depth} parameter that governs how far down the tree the algorithm should explore. We generate node values in Minimax according to this heuristic:

\begin{itemize}

\item For length of streak, each streak has value $x * 2^(x-1)$, where x is the length of the streak
\item For opponent streaks, each node has value $(x *  2^x) / 6$, where x is the length of the streak

We decided upon these values \textbf{LUCY}!!!!

\end{itemize}

We also explore an extension to Minimax, called \textbf{alpha-beta pruning}, motivated by the idea that often Minimax explores more branches than is required. For example, if it needs to expand down to depth 3, but is currently at depth 2, it will continue expanding all branches to depth 3 even if one branch clearly would lead to a loss. In alpha-beta pruned Minimax, the tree is pruned to only continue investigating good paths. The algorithm controls the range of values the tree should continue searching down, and we initialize this range to $\pm 1000$ for generality.

\end{enumerate}

\subsubsection{Q Learning}

The first reinforcement learning approach we looked at was Q Learning, an off-policy TD control algorithm that we investigated in Practical 1. The straightforward version of Q Learning we implemented in Practical 1 did not seem like it could have any chance of performing as well as an AI algorithm such as Minimax, so we additionally implemented a number of extensions to Q Learning.

\begin{enumerate}
\item We replaced $\epsilon$-greedy with the \textbf{VDBE-Softmax} policy described in [Tokic, 2011]. When evaluated on the Gridworld, VDBE-Softmax outperformed $\epsilon$-greedy in cumulative reward substantially, so we thought it could potentially do better in Connect-N as well.
\item We implemented \textbf{eligibility traces}, as described in the Sutton and Barto textbook and as we covered in class discussions, as it can help the learner learn more efficiently. Unfortunately, the complete eligibility traces algorithm iterates over all states and actions, which was computationally infeasible in this game, as the states represent all possible versions of the Connect N board state. Instead, we implement a limited version of eligibility traces, where the algorithm updates $Q(s, a)$ and $e(s, a)$ for each board state it currently knows and all board states that can be reached by taking one action from the current board state.
\item \textbf{LUCY}- add known states and shaping rewards

\end{enumerate}

\begin{wrapfigure}{l}{5cm}
	\includegraphics[scale=.16]{transform}
\end{wrapfigure}

The Q learner continues to keep an action value table (as in Practical 1), with a slightly different data structure. The Q learner takes the board state and hashes it into a string, where 0 represents an empty spot and 1 and -1 represent self and opponent tokens. It then adds the string board state as a key into a dictionary. For computational reasons, instead of adding all of the possible board states to the dictionary, the dictionary only ever holds board states that it has seen. 

To pre-train the Q learner, we pass the board state between different games, allowing the Q learner to have a better, more competitive knowledge of what actions are good or bad.

\subsubsection{Monte Carlo Tree Search- andre} 


\section{detailed description of your results/comparisons- everyone- wait for results}

\section{thorough description of why you got the results you did, a few pages- everyone- wait for results}


\section{Future Work}

Interesting research questions for the future could include:

\begin{enumerate}
\item Exploring extensions to MCTS, such as a better default policy (perhaps estimation of a value for each board state, and selecting the action that leads to the board state with highest value)
\item Exploring large N, where computational complexity of solving MCTS and Minimax could potentially allow Q learning to perform competitively (as MCTS iteration number would be low and Minimax depth parameter would be low)
\item Adding stochasticity to the board, such as dropping the bottom row with some probability or having a probability of taking the wrong action
\item Coding the board state as a POMDP, with information such as the number of tokens in a row, but not knowing exactly where the tokens are 
\end{enumerate}



\end{document}







