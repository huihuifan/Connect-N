{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andre's Code (04/26/15 - 9pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import math\n",
    "from random import randint\n",
    "import itertools\n",
    "import random\n",
    "import copy\n",
    "from copy import deepcopy \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ken's break ties function.\n",
    "def argmax_breaking_ties_randomly(x):\n",
    "    max_value = np.max(x)\n",
    "    indices_with_max_value = np.flatnonzero(x == max_value)\n",
    "    return np.random.choice(indices_with_max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConnectN:\n",
    "    \"\"\"\n",
    "    Connect N game simulator for two players, 1 and -1.\n",
    "    \n",
    "    Inputs:\n",
    "    Grid size- creates a grid size x grid size square board\n",
    "    N- number of tokens a player must connect to win the game\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size, n):\n",
    "        self.n = n\n",
    "        self.grid_size = grid_size\n",
    "        self.grid = np.zeros([grid_size,grid_size])\n",
    "        self.finished = 0\n",
    "        self.turn_num = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.__init__(self.grid_size, self.n)\n",
    "\n",
    "    def check_win(self, col, row, player):\n",
    "        \"\"\"\n",
    "        Checks if given player has connected N tokens.\n",
    "        \"\"\"\n",
    "        for i in range(0, self.n):\n",
    "            if sum(self.grid[col, row - i:row - i + self.n]) == self.n*player:\n",
    "                self.finished = 1\n",
    "                return 1\n",
    "            if sum(self.grid[col - i: col - i + self.n, row]) == self.n*player:\n",
    "                self.finished = 1\n",
    "                return 1\n",
    "            if col - i >= 0 and col - i + self.n - 1 < self.grid_size and row - i >= 0 and row - i + self.n - 1 < self.grid_size:\n",
    "                if sum([self.grid[col - i + x, row - i + x] for x in range(0, self.n)]) == self.n*player:\n",
    "                    self.finished = 1\n",
    "                    return 1\n",
    "            if col - i >= 0 and col - i + self.n - 1 < self.grid_size and row + i >= self.n - 1 and row + i < self.grid_size:\n",
    "                if sum([self.grid[col - i + x, row + i - x] for x in range(0, self.n)]) == self.n*player:\n",
    "                    self.finished = 1\n",
    "                    return 1\n",
    "        return 0\n",
    "\n",
    "    def move(self, col, player):\n",
    "        \"\"\"\n",
    "        Given player and column to move in, modifies board and increments the turn counter.\n",
    "        \n",
    "        Returns a tuple, where first value is return message and second value is reward.\n",
    "        \"\"\"\n",
    "        self.turn_num += 1\n",
    "        \n",
    "        if self.finished == 1:\n",
    "            return 1, 50\n",
    "        sum_col = np.sum([abs(x) for x in self.grid[col]])\n",
    "        if sum_col == self.grid_size:\n",
    "            return -1, -1\n",
    "        self.grid[col, sum_col] = player\n",
    "        if self.check_win(col, sum_col, player) == 1:\n",
    "            return 1, 50\n",
    "        return 0, 0\n",
    "    \n",
    "    def turn(self):\n",
    "        \"\"\"\n",
    "        Returns which player's turn it is. First turn is player 1, second turn is player -1.\n",
    "        \"\"\"\n",
    "        if self.turn_num%2 == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def next_possible_moves(self):\n",
    "        \"\"\"\n",
    "        Returns array of possible columns for a next move\n",
    "        \"\"\"\n",
    "        columns = []\n",
    "        \n",
    "        for i in xrange(0, self.grid_size):\n",
    "            if (0 in self.grid[i]):\n",
    "                columns.append(i)\n",
    "                \n",
    "        return columns\n",
    "    \n",
    "    def all_tokens_placed(self):\n",
    "        \"\"\"\n",
    "        Returns location of all tokens (column, row) that have been placed\n",
    "        \"\"\"\n",
    "        all_tokens = []\n",
    "        \n",
    "        for col in xrange(0, self.grid_size):\n",
    "            for row in xrange(0, self.grid_size): \n",
    "                if self.grid[col][row] != 0:\n",
    "                    all_tokens.append({\"location\": [col, row], \"player\": self.grid[col][row]})\n",
    "                    \n",
    "        return all_tokens\n",
    "    \n",
    "    def is_empty(self, col, row):\n",
    "        \"\"\"\n",
    "        Returns if a given spot (column, row) is empty\n",
    "        \"\"\"\n",
    "        return self.grid[col][row] == 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Following streak functions check if player has token streak in the four possible win directions\n",
    "    \"\"\"\n",
    "    def streakVertical(self, board, col, row, player):\n",
    "        if row > len(board[col]) - self.n:\n",
    "            return 0\n",
    "        for i in range(0,self.n):\n",
    "            if board[col][row + i] == -1*player:\n",
    "                return 0\n",
    "            if board[col][row + i] == 0:\n",
    "                return i\n",
    "        return self.n\n",
    "\n",
    "    def streakHorizontal(self, board, col, row, player):\n",
    "        if col > len(board) - self.n:\n",
    "            return 0\n",
    "        for i in range(0,self.n):\n",
    "            if board[col + i][row] == -1*player:\n",
    "                return 0\n",
    "            if board[col + i][row] == 0:\n",
    "                return i\n",
    "        return self.n\n",
    "    \n",
    "    def streakDiagonalUp(self, board, col, row, player):\n",
    "        if row > len(board[col]) - self.n or col > len(board) - self.n:\n",
    "            return 0\n",
    "        for i in range(0,self.n):\n",
    "            if board[col + i][row + i] == -1*player:\n",
    "                return 0\n",
    "            if board[col + i][row + i] == 0:\n",
    "                return i\n",
    "        return self.n\n",
    "    \n",
    "    def streakDiagonalDown(self, board, col, row, player):\n",
    "        if row < self.n or col > len(board) - self.n:\n",
    "            return 0\n",
    "        for i in range(0,self.n):\n",
    "            if board[col + i][row - i] == -1*player:\n",
    "                return 0\n",
    "            if board[col + i][row - i] == 0:\n",
    "                return i\n",
    "        return self.n\n",
    "    \n",
    "    def print_grid(self):\n",
    "        print(np.rot90(self.grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_game(board, p1, p2, q=False):\n",
    "    \"\"\"\n",
    "    Runs Connect 4 game given simulator object and two agents (players)\n",
    "    \"\"\"\n",
    "    reward = None\n",
    "    \n",
    "    if q == True:\n",
    "        while True:\n",
    "            print(\"p1\")\n",
    "            p1move = p1.interact(reward, board)\n",
    "            print(p1move)\n",
    "            if (p1move is None):\n",
    "                board.print_grid()\n",
    "                print(\"error player 1 a\")\n",
    "                return -1\n",
    "            p1result, reward = board.move(p1move, 1)\n",
    "            print p1result\n",
    "            if (p1result == 1):\n",
    "                print(\"player 1\")\n",
    "                board.print_grid()\n",
    "                return 1\n",
    "            elif (p1result == -1):\n",
    "                board.print_grid()\n",
    "                print(\"error player 1 b\")\n",
    "                return -1\n",
    "            print(\"p2\")\n",
    "            p2move = p2.calc_next_move()\n",
    "            print(p2move)\n",
    "            if (p2move is None):\n",
    "                board.print_grid()\n",
    "                print(\"error player 2\")\n",
    "                return -1\n",
    "            p2result = board.move(p2move, -1)\n",
    "            print p2result\n",
    "            if (p2result[0] == 1):\n",
    "                print(\"player 2\")\n",
    "                board.print_grid()\n",
    "                return 1\n",
    "            elif (p2result[0] == -1):\n",
    "                board.print_grid()\n",
    "                print(\"error player 2\")\n",
    "                return -1\n",
    "    \n",
    "    else:\n",
    "        while True:\n",
    "            print(\"p1\")\n",
    "            p1move = p1.calc_next_move()\n",
    "            print(p1move)\n",
    "            if (p1move is None):\n",
    "                board.print_grid()\n",
    "                print(\"error\")\n",
    "                return -1\n",
    "            p1result = board.move(p1move, 1)\n",
    "            print p1result\n",
    "            if (p1result[0] == 1):\n",
    "                print(\"player 1\")\n",
    "                board.print_grid()\n",
    "                return 1\n",
    "            elif (p1result[0] == -1):\n",
    "                board.print_grid()\n",
    "                print(\"error\")\n",
    "                return -1\n",
    "            print(\"p2\")\n",
    "            p2move = p2.calc_next_move()\n",
    "            print(p2move)\n",
    "            if (p2move is None):\n",
    "                board.print_grid()\n",
    "                print(\"error\")\n",
    "                return -1\n",
    "            p2result = board.move(p2move, -1)\n",
    "            print p2result\n",
    "            if (p2result[0] == 1):\n",
    "                print(\"player 2\")\n",
    "                board.print_grid()\n",
    "                return 1\n",
    "            elif (p2result[0] == -1):\n",
    "                board.print_grid()\n",
    "                print(\"error\")\n",
    "                return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimax with depth past 6 takes a long time to make a move, seems not necessary\n",
    "# Minimax depth 5 is \"hard\"\n",
    "# Minimax depth 3 is \"medium\"\n",
    "# Minimax depth 1 is \"easy\"\n",
    "\n",
    "class Minimax_Learner(object):\n",
    "    \"\"\"\n",
    "    Implementation of AI algorithm Minimax with static evaluator \n",
    "    \n",
    "    Inputs:\n",
    "    Connect N board\n",
    "    Depth- Minimax Learner will build tree of next possible moves to that depth\n",
    "    N- number of tokens that need to be connected for a player to win\n",
    "    Player- player number, either 1 or -1\n",
    "    Algorithm- either \"minimax\" or \"ab\" for alpha beta pruned minimax\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, board, depth, n, player, alg):\n",
    "        self.board = board\n",
    "        self.depth = depth\n",
    "        self.num_states = board.grid_size\n",
    "        self.player = player\n",
    "        self.n = n\n",
    "        self.alg = alg\n",
    "\n",
    "    def value(self, board):\n",
    "        \"\"\"\n",
    "        Calculates value of board states\n",
    "        \"\"\"\n",
    "        val = 0\n",
    "        conversion = [int(i*math.pow(2, i))/2 for i in range(0, self.n+1)]\n",
    "        conversion[self.n] = 20000000\n",
    "        conversion_other = [i*int(math.pow(2, i))/6 for i in range(0, self.n+1)]\n",
    "        conversion_other[self.n] = 1000000\n",
    "        for i in range(0, len(board)):\n",
    "            for j in range(0, len(board[0])):\n",
    "                temp = self.board.streakVertical(board, i, j, self.player)\n",
    "                if temp == self.n:\n",
    "                    return conversion[temp]\n",
    "                val += conversion[temp]\n",
    "                \n",
    "                temp = self.board.streakHorizontal(board, i, j, self.player)\n",
    "                if temp == self.n:\n",
    "                    return conversion[temp]\n",
    "                val += conversion[temp]    \n",
    "                \n",
    "                temp = self.board.streakDiagonalUp(board, i, j, self.player)\n",
    "                if temp == self.n:\n",
    "                    return conversion[temp]\n",
    "                val += conversion[temp]\n",
    "                \n",
    "                temp = self.board.streakDiagonalDown(board, i, j, self.player)\n",
    "                if temp == self.n:\n",
    "                    return conversion[temp]\n",
    "                val += conversion[temp]\n",
    "                \n",
    "                \n",
    "                temp = self.board.streakVertical(board, i, j, -1*self.player)\n",
    "                if temp == self.n:\n",
    "                    return -1*conversion_other[temp]\n",
    "                val -= conversion[temp]\n",
    "\n",
    "                temp = self.board.streakHorizontal(board, i, j, -1*self.player)\n",
    "                if temp == self.n:\n",
    "                    return -1*conversion_other[temp]\n",
    "                val -= conversion[temp]\n",
    "                temp = self.board.streakDiagonalUp(board, i, j, -1*self.player)\n",
    "                if temp == self.n:\n",
    "                    return -1*conversion_other[temp]\n",
    "                val -= conversion[temp]\n",
    "                temp = self.board.streakDiagonalDown(board, i, j, -1*self.player)\n",
    "                if temp == self.n:\n",
    "                    return -1*conversion_other[temp]\n",
    "                val -= conversion[temp]\n",
    "\n",
    "        return val\n",
    "        \n",
    "    def create_tree(self, node, depth, player, move):\n",
    "        \"\"\"\n",
    "        Creates tree of next possible moves\n",
    "        \n",
    "        Each node is a dict of node value, children, the board state, which player's turn it would be, and move\n",
    "        \"\"\"\n",
    "        if depth == 0:\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "            tree = {\"value\": 0, \"children\": [], \"board\": node, \"player\": player, \"move\": move}\n",
    "\n",
    "            next_moves = node.next_possible_moves()\n",
    "\n",
    "            for move in next_moves:\n",
    "                board_copy = copy.deepcopy(node)\n",
    "                board_copy.move(move, player)\n",
    "                new_child = self.create_tree(board_copy, depth-1, -1*player, move)\n",
    "                \n",
    "                if new_child != None:\n",
    "                    tree[\"children\"].append(new_child)\n",
    "\n",
    "            return tree\n",
    "\n",
    "    def children(self, node):\n",
    "        \"\"\" \n",
    "        returns children of a node\n",
    "        \"\"\"\n",
    "        return node[\"children\"]\n",
    "   \n",
    "    def leaf(self, node):\n",
    "        \"\"\"\n",
    "        returns if current node is a leaf (i.e. no children)\n",
    "        \"\"\"\n",
    "        return len(self.children(node)) == 0\n",
    "        \n",
    "    def max_node(self, node):\n",
    "        \"\"\"\n",
    "        returns true if node is a max node\n",
    "        \"\"\"\n",
    "        return node[\"player\"] == self.player\n",
    "        \n",
    "    def evaluate(self, node):\n",
    "        \"\"\"\n",
    "        Static evaluator function to return a value between Loss and Win for intermediate game\n",
    "        positions, larger if the position is better for the current player.\n",
    "        If depth limit of the search is exceeded, is applied to remaining nodes as if\n",
    "        they were leaves. \n",
    "        \n",
    "        We calculate the rating by checking each token already placed, and \n",
    "        checking how many possible ways to connect N there are\n",
    "        \"\"\"\n",
    "        node[\"value\"] = self.value(node[\"board\"].grid)\n",
    "        return node[\"value\"]       \n",
    "\n",
    "    def minimax(self, node, depth):\n",
    "        \"\"\" \n",
    "        Recursive implementation of Minimax algorithm using pseudocode from: \n",
    "        https://www.cs.cornell.edu/courses/cs312/2002sp/lectures/rec21.htm\n",
    "        \"\"\"\n",
    "        if self.leaf(node) or depth == 0:\n",
    "            return self.evaluate(node)\n",
    "        \n",
    "        if self.max_node(node):\n",
    "            # L = -infinity\n",
    "            current_node_value = -1000000000\n",
    "            for child in self.children(node):\n",
    "                next_node_value = self.minimax(child, depth-1)\n",
    "                if current_node_value < next_node_value:\n",
    "                    current_node_value = next_node_value\n",
    "            node[\"value\"] = current_node_value\n",
    "            return current_node_value\n",
    "        \n",
    "        if not self.max_node(node):\n",
    "            # W = +infinity\n",
    "            current_node_value = 10000000000\n",
    "            for child in self.children(node):\n",
    "                next_node_value = self.minimax(child, depth-1)\n",
    "                if next_node_value < current_node_value:\n",
    "                    current_node_value = next_node_value\n",
    "            node[\"value\"] = current_node_value\n",
    "            return current_node_value\n",
    "\n",
    "        \n",
    "    def ab_minimax(self, node, depth, min_val, max_val):\n",
    "        \"\"\" \n",
    "        Implementation of Minimax with Alpha Beta Pruning\n",
    "        \n",
    "        In contrast to previous minimax algorithm, must now input min_val and max_val as well\n",
    "        \"\"\"\n",
    "        if self.leaf(node) or depth == 0:\n",
    "            return self.evaluate(node)\n",
    "        \n",
    "        if self.max_node(node):\n",
    "            current_node_value = min_val\n",
    "            for child in self.children(node):\n",
    "                next_node_value = self.ab_minimax(child, depth-1, current_node_value, max_val)\n",
    "                if current_node_value < next_node_value:\n",
    "                    current_node_value = next_node_value\n",
    "                if current_node_value > max_val:\n",
    "                    return max_val\n",
    "            node[\"value\"] = current_node_value\n",
    "            return current_node_value\n",
    "        \n",
    "        if not self.max_node(node):\n",
    "            current_node_value = max_val\n",
    "            for child in self.children(node):\n",
    "                next_node_value = self.ab_minimax(child, depth-1, min_val, current_node_value)\n",
    "                if next_node_value < current_node_value:\n",
    "                    current_node_value = next_node_value\n",
    "                if current_node_value < min_val:\n",
    "                    return min_val\n",
    "            node[\"value\"] = current_node_value\n",
    "            return current_node_value\n",
    "        \n",
    "    def calc_next_move(self):\n",
    "        \"\"\"\n",
    "        Calculate Minimax's Learners optimal next move\n",
    "        \"\"\"\n",
    "        current_tree = self.create_tree(self.board, self.depth, self.player, None)\n",
    "        \n",
    "        if self.alg == \"minimax\":\n",
    "            top_val = self.minimax(current_tree, self.depth)\n",
    "        elif self.alg == \"ab\":\n",
    "            top_val = self.ab_minimax(current_tree, self.depth, -100000, 100000)\n",
    "            \n",
    "        print \"this is top_val\", top_val\n",
    "                \n",
    "        for child in current_tree[\"children\"]:\n",
    "            if child[\"value\"] == top_val:\n",
    "                print \"i'm here\"\n",
    "                return child[\"move\"]\n",
    "        \n",
    "        top_val = np.min([x[\"value\"] for x in current_tree[\"children\"]])\n",
    "        for child in current_tree[\"children\"]:\n",
    "            if child[\"value\"] == top_val:\n",
    "                print \"i'm here\"\n",
    "                return child[\"move\"]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Random_Learner(object):\n",
    "    \"\"\"\n",
    "    Implementation of Connect 4 agent that takes random moves at each action step\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, board):\n",
    "        self.board = board\n",
    "        \n",
    "\n",
    "    def calc_next_move(self):\n",
    "        moves = self.board.next_possible_moves()\n",
    "        return moves[random.randint(0, len(moves) - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Extending Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Extend_Learner(object):\n",
    "    \"\"\"\n",
    "    Implementation of naive algorithm that tries to extend longest streak. Prioritizes vertical streaks.\n",
    "    \n",
    "    Inputs:\n",
    "    Connect N board\n",
    "    Depth- Minimax Learner will build tree of next possible moves to that depth\n",
    "    N- number of tokens that need to be connected for a player to win\n",
    "    Player- player number, either 1 or -1\n",
    "    Algorithm- either \"minimax\" or \"ab\" for alpha beta pruned minimax\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, board, depth, n, player):\n",
    "        self.board = board\n",
    "        self.depth = depth\n",
    "        self.num_states = board.grid_size\n",
    "        self.player = player\n",
    "        self.n = n\n",
    "        \n",
    "\n",
    "    def get_max(self, d):\n",
    "        maxval = max(d)\n",
    "        options = [x for x in range(0, len(d)) if d[x] == maxval]\n",
    "        return options[random.randint(0, len(options) - 1)]\n",
    "\n",
    "    def calc_next_move(self):\n",
    "        \"\"\"\n",
    "        Calculate Minimax's Learners optimal next move\n",
    "        \"\"\"\n",
    "        max_len = 0\n",
    "        board = self.board.grid\n",
    "        max_cols = [0 for i in range(0, len(board))]\n",
    "        \n",
    "        \n",
    "        for i in range(0, len(board)):\n",
    "            for j in range(0, len(board[0])):\n",
    "                temp_len = self.board.streakVertical(board, i, j, self.player)\n",
    "                if temp_len > max_len:\n",
    "                    max_len = temp_len\n",
    "                    max_cols = [0 for k in range(0, len(board))]\n",
    "                    max_cols[i] += 2\n",
    "                elif temp_len == max_len:\n",
    "                    max_cols[i] += 2\n",
    "                    \n",
    "                temp_len = self.board.streakHorizontal(board, i, j, self.player)\n",
    "                if temp_len > max_len:\n",
    "                    max_len = temp_len\n",
    "                    max_cols = [0 for k in range(0, len(board))]\n",
    "                    max_cols[i + temp_len] += 1\n",
    "                elif temp_len == max_len:\n",
    "                    max_cols[i + temp_len] += 1\n",
    "\n",
    "                temp_len = self.board.streakDiagonalUp(board, i, j, self.player)\n",
    "                if temp_len > max_len:\n",
    "                    max_len = temp_len\n",
    "                    max_cols = [0 for k in range(0, len(board))]\n",
    "                    max_cols[i + temp_len] += 1\n",
    "                elif temp_len == max_len:\n",
    "                    max_cols[i + temp_len] += 1\n",
    "                \n",
    "                temp_len = self.board.streakDiagonalDown(board, i, j, self.player)\n",
    "                if temp_len > max_len:\n",
    "                    max_len = temp_len\n",
    "                    max_cols = [0 for k in range(0, len(board))]\n",
    "                    max_cols[i + temp_len] += 1\n",
    "                elif temp_len == max_len:\n",
    "                    max_cols[i + temp_len] += 1\n",
    "        \n",
    "        return self.get_max(max_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_to_key(grid):\n",
    "    \"\"\"\n",
    "    Converts ConnectN grid into string for dict indexing\n",
    "    \"\"\"\n",
    "\n",
    "    key = \"\"\n",
    "\n",
    "    for row in np.rot90(grid):\n",
    "        for column in row:\n",
    "            key += str(int(column))\n",
    "\n",
    "    return key\n",
    "\n",
    "\n",
    "class ConnectDict(dict):\n",
    "    \"\"\"\n",
    "    Creates a Custom Dict that inherits from Python's native dict.\n",
    "    Takes in a number of states.\n",
    "    Adds keys to dict each time lookup is necessary to avoid full dict initialization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_states, *arg, **kw):\n",
    "        self.num_states = num_states\n",
    "        super(ConnectDict, self).__init__(*arg, **kw)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        if not dict.__contains__(self, key):\n",
    "            dict.__setitem__(self, key, np.zeros(self.num_states))\n",
    "        return dict.__getitem__(self, key)\n",
    "    \n",
    "\n",
    "class TD_Learner(object):\n",
    "    \"\"\"\n",
    "    Base class for Temporal Difference Learners, like Sarsa and Q learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, task, value_table=None, epsilon=.1, discount_factor=.9, learning_rate=.5, player=1, trace_size=.1):\n",
    "        \n",
    "        self.num_states = task.grid_size\n",
    "        self.num_actions = task.grid_size\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if value_table == None:\n",
    "            self.value_table = ConnectDict(self.num_states)\n",
    "        else:\n",
    "            self.value_table = value_table\n",
    "            \n",
    "        self.e = ConnectDict(self.num_states)\n",
    "        self.player = player\n",
    "        self.trace_size = trace_size\n",
    "        self.last_board_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.last_board_state = None\n",
    "#         self.last_action = None\n",
    "    \n",
    "    def softmax(self, next_board_state):\n",
    "        \"\"\"\n",
    "        Implementation of Softmax Policy, which weights towards better actions rather\n",
    "        than sampling uniformly across all possible actions (epsilon-greedy)\n",
    "        \"\"\"\n",
    "        \n",
    "        def weighted_pick(weights,n_picks):\n",
    "            t = np.cumsum(weights)\n",
    "            s = sum(weights)\n",
    "            return np.searchsorted(t,rand(n_picks)*s)\n",
    "        \n",
    "        tau = .5\n",
    "        key_val = grid_to_key(next_board_state.grid)\n",
    "        \n",
    "        vals = self.value_table[key_val]\n",
    "        num = ([math.e**(float(x)/tau) for x in vals])\n",
    "        \n",
    "        probs = [x/sum(num) for x in num]\n",
    "        best_action = weighted_pick(probs, 1)\n",
    "\n",
    "        return best_action[0]\n",
    "    \n",
    "    \n",
    "class Q_Learner(TD_Learner):\n",
    "    \"\"\"\n",
    "    Implementation of Q Learning, inheriting from TD Learner base class. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, task, value_table, known_states, epsilon=.1, discount_factor=.9, learning_rate=.5, player=1, trace_size=.1):   \n",
    "        TD_Learner.__init__(self, task, value_table, epsilon, discount_factor, learning_rate, player, trace_size) \n",
    "        self.known_states = known_states\n",
    "        \n",
    "\n",
    "    def interact(self, reward, next_board_state):\n",
    "        if reward is None:\n",
    "            # Approximation of known states. Since too many states, instead, given a board position, \n",
    "            # explore possible moves and give 15 reward to creating streaks of length 3 or 4 and \n",
    "            # 20 reward for preventing an opponent win.\n",
    "            if (self.known_states):\n",
    "                for col in task.next_possible_moves():\n",
    "                    row = np.sum([abs(x) for x in next_board_state.grid[col]])\n",
    "                    if next_board_state.streakVertical(next_board_state.grid, col, row - 2, self.player) >= 2:\n",
    "                        self.value_table[grid_to_key(next_board_state.grid)][col] = 15\n",
    "                    temp_board = deepcopy(next_board_state.grid)\n",
    "                    temp_board[col][row] = self.player\n",
    "                    for i in range(0, 4):\n",
    "                        if next_board_state.streakHorizontal(temp_board, col - i, row, self.player) >= 3:\n",
    "                            self.value_table[grid_to_key(next_board_state.grid)][col] = 15\n",
    "                        if next_board_state.streakDiagonalUp(temp_board, col - i, row - i, self.player) >= 3:\n",
    "                            self.value_table[grid_to_key(next_board_state.grid)][col] = 15\n",
    "                        if next_board_state.streakDiagonalDown(temp_board, col - i, row + i, self.player) >= 3:\n",
    "                            self.value_table[grid_to_key(next_board_state.grid)][col] = 15\n",
    "                            \n",
    "                    if next_board_state.streakVertical(next_board_state.grid, col, row - 3, -self.player) == 3:\n",
    "                        self.value_table[grid_to_key(next_board_state.grid)][col] = 20\n",
    "                    temp_board = deepcopy(next_board_state.grid)\n",
    "                    temp_board[col][row] = -1*self.player\n",
    "                    for i in range(0, 4):\n",
    "                        if next_board_state.streakHorizontal(temp_board, col - i, row, -1*self.player) == 4:\n",
    "                            self.value_table[grid_to_key(next_board_state.grid)][col] = 20\n",
    "                        if next_board_state.streakDiagonalUp(temp_board, col - i, row - i, -1*self.player) == 4:\n",
    "                            self.value_table[grid_to_key(next_board_state.grid)][col] = 20\n",
    "                        if next_board_state.streakDiagonalDown(temp_board, col - i, row + i, -1*self.player) == 4:\n",
    "                            self.value_table[grid_to_key(next_board_state.grid)][col] = 20\n",
    "\n",
    "            next_action = self.softmax(next_board_state)\n",
    "\n",
    "            self.last_board_state = next_board_state.grid\n",
    "            self.last_action = next_action\n",
    "            return self.last_action\n",
    "                \n",
    "        if reward == 50:\n",
    "            delta = delta = reward - self.value_table[grid_to_key(self.last_board_state)][self.last_action]\n",
    "            self.value_table[grid_to_key(self.last_board_state)][self.last_action] += self.learning_rate * delta\n",
    "            \n",
    "            return self.last_action\n",
    "        \n",
    "        \"\"\"\n",
    "        VDBE-Softmax policy. If draw < epsilon, perform Softmax. Else do best action.\n",
    "        \"\"\"\n",
    "        draw = np.random.uniform(0,1,1)\n",
    "\n",
    "        if draw < self.epsilon:\n",
    "            next_action = self.softmax(next_board_state)\n",
    "        else:\n",
    "            next_action = np.argmax(self.value_table[grid_to_key(next_board_state.grid)])\n",
    "\n",
    "        # Update value function.\n",
    "        delta = reward + self.discount_factor * np.amax(self.value_table[grid_to_key(next_board_state.grid)]) - self.value_table[grid_to_key(self.last_board_state)][self.last_action]\n",
    "        self.value_table[grid_to_key(self.last_board_state)][self.last_action] += self.learning_rate * delta\n",
    "        \n",
    "        # Update eligibility traces (Watson's Q(lambda))\n",
    "        self.e[grid_to_key(self.last_board_state)][self.last_action] += 1\n",
    "\n",
    "        # Eligibility traces\n",
    "        # Note that here we do not implement classic eligibility traces, which iterate over all state, action pairs\n",
    "        # Instead we consider all next possible board states and update those (for easier computation)\n",
    "        next_possible_moves = next_board_state.next_possible_moves()\n",
    "        next_possible_boards = []\n",
    "        \n",
    "        for i in next_possible_moves:\n",
    "            temp_board = deepcopy(next_board_state)\n",
    "            temp_board.move(next_action, self.player)\n",
    "            next_possible_boards.append(temp_board)\n",
    "            \n",
    "        for board in next_possible_boards:\n",
    "            valid_actions = board.next_possible_moves()\n",
    "            for action in valid_actions:\n",
    "                self.value_table[grid_to_key(board.grid)][action] += self.learning_rate * delta \\\n",
    "                                                                    * self.e[grid_to_key(board.grid)][action]\n",
    "                if self.last_action == action:\n",
    "                    self.e[grid_to_key(board.grid)][action] = self.discount_factor * self.trace_size \\\n",
    "                                                                    * self.e[grid_to_key(board.grid)][action]\n",
    "                else:\n",
    "                    self.e[grid_to_key(board.grid)][action] = 0\n",
    "                    \n",
    "        self.last_board_state = next_board_state.grid\n",
    "        self.last_action = next_action\n",
    "\n",
    "        if next_board_state.simulate_move(self.last_action, self.player) == 1:\n",
    "            self.last_action = self.softmax(next_board_state)\n",
    "            \n",
    "        return self.last_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Define a Tree Data Structure for MCTS\n",
    "    \"\"\"\n",
    "    def __init__(self, state, parent, action_taken):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.actions = []\n",
    "        self.action_taken = action_taken\n",
    "        self.children = []\n",
    "        self.total_reward = 0\n",
    "        self.total_visit_count = 0.000001\n",
    "        \n",
    "        \n",
    "class MCTS(object):\n",
    "    \"\"\"\n",
    "    Monte Carlo Tree Search \n",
    "    UCT algorithm from \"A Survey of Monte Carlo Tree Search Methods\n",
    "    \n",
    "    Parameters: board, max number of iterations, C (exploration term)\n",
    "    \n",
    "    Node is Agent's move, next move is enemy\n",
    "    \"\"\"    \n",
    "    def __init__(self, board, max_iter, C):\n",
    "        self.board = board\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        \n",
    "    def reset(self):\n",
    "        self.__init__(self.max_iter, self.C)\n",
    "        \n",
    "    def calc_next_move(self):\n",
    "        return self.uct_search(self.board)\n",
    "        \n",
    "    def full_check_win(self,board):\n",
    "        for col in xrange(0,board.grid_size):\n",
    "            for row in xrange(0,board.grid_size):\n",
    "                if board.check_win(col, row, 1) == 1:\n",
    "                    return True\n",
    "                if board.check_win(col, row, -1) == 1:\n",
    "                    return True\n",
    "        return False\n",
    "        \n",
    "    def uct_search(self,board):\n",
    "        iterator = 0\n",
    "        root = Node(board, None, None)\n",
    "        while iterator < self.max_iter:\n",
    "            c_node = self.tree_policy(root)\n",
    "            d = self.default_policy(c_node.state)\n",
    "            self.backup(c_node,d)\n",
    "            iterator = iterator + 1\n",
    "        return self.best_child(root,0)\n",
    "        \n",
    "    def tree_policy(self,node):\n",
    "        x = node\n",
    "        while self.full_check_win(x.state) == False and x.state.next_possible_moves() != []:\n",
    "            if list(set(x.state.next_possible_moves()) - set(x.actions)) != []:\n",
    "                return self.expand(x)\n",
    "            else: \n",
    "                x = x.children[self.best_child(x,self.C)]\n",
    "        return x\n",
    "    \n",
    "    def expand(self,node):\n",
    "        untried = list(set(node.state.next_possible_moves()) - set(node.actions))\n",
    "        if untried != []: \n",
    "            if node.state.turn() == 1:\n",
    "                child = copy.deepcopy(node)\n",
    "                child.state.move(untried[0],1)\n",
    "                node.children.append(Node(child.state, node, untried[0]))\n",
    "                node.actions.append(untried[0])\n",
    "                return node.children[-1]\n",
    "            if node.state.turn() == -1:\n",
    "                child = copy.deepcopy(node)\n",
    "                child.state.move(untried[0],-1)\n",
    "                node.children.append(Node(child.state, node, untried[0]))\n",
    "                node.actions.append(untried[0])\n",
    "                return node.children[-1]\n",
    "        return \n",
    "\n",
    "    \n",
    "    def best_child(self,node,c):\n",
    "        child_vals = [((x.total_reward)/(x.total_visit_count) + c * np.sqrt(2*np.log(node.total_visit_count)/x.total_visit_count)) for x in node.children]\n",
    "        best_inx = argmax_breaking_ties_randomly(child_vals)  \n",
    "        best_c = node.children[best_inx]\n",
    "        #print(child_vals)\n",
    "        return best_c.action_taken\n",
    "        \n",
    "    def default_policy(self,board):\n",
    "        if board.turn() == 1:\n",
    "            # assumes that agent is player 1\n",
    "            board2 = copy.deepcopy(board)\n",
    "            if self.full_check_win(board2) == True:\n",
    "                return 50\n",
    "            while True:\n",
    "                if board2.turn() == -1:\n",
    "                    # imagined enemy\n",
    "                    action2 = np.random.choice(board2.next_possible_moves())\n",
    "                    board2.move(action2, -1)\n",
    "                    if self.full_check_win(board2) == True:\n",
    "                        return -50\n",
    "                    if board2.next_possible_moves() == []:\n",
    "                        return 0\n",
    "                # agent\n",
    "                if board2.turn() == 1:\n",
    "                    action = np.random.choice(board2.next_possible_moves())\n",
    "                    board2.move(action, 1)\n",
    "                    if self.full_check_win(board2) == True:\n",
    "                        return 50\n",
    "                    if board2.next_possible_moves() == []:\n",
    "                        return 0        \n",
    "        if board.turn() == -1:\n",
    "            # assumes that agent is player -1\n",
    "            board2 = copy.deepcopy(board)\n",
    "            if self.full_check_win(board2) == True:\n",
    "                return 50\n",
    "            while True:\n",
    "                if board2.turn() == 1:\n",
    "                    # imagined enemy\n",
    "                    action2 = np.random.choice(board2.next_possible_moves())\n",
    "                    board2.move(action2, 1)\n",
    "                    if self.full_check_win(board2) == True:\n",
    "                        return -50\n",
    "                    if board2.next_possible_moves() == []:\n",
    "                        return 0\n",
    "                # agent\n",
    "                if board2.turn() == -1:\n",
    "                    action = np.random.choice(board2.next_possible_moves())\n",
    "                    board2.move(action, -1)\n",
    "                    if self.full_check_win(board2) == True:\n",
    "                        return 50\n",
    "                    if board2.next_possible_moves() == []:\n",
    "                        return 0\n",
    "            \n",
    "    def backup(self,node,d):\n",
    "        v = node \n",
    "        while v != None:\n",
    "            v.total_visit_count = v.total_visit_count + 1\n",
    "            v.total_reward = v.total_reward + d\n",
    "            v = v.parent\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
