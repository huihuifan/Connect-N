{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from simulator import *\n",
    "from agents import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "x = ConnectN(7, 4)\n",
    "x.print_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_Q_learner(num_trials = 1000, k=2, n=4, grid_size = 7):\n",
    "    \"\"\"\n",
    "    Trains the Q Learner against Minimax Depth k\n",
    "    \n",
    "    Inputs:\n",
    "    Number of games to play\n",
    "    On grid_size x\n",
    "    N tokens to connect\n",
    "    \n",
    "    Outputs:\n",
    "    Q Learner value table after training\n",
    "    \"\"\"\n",
    "    depth = k\n",
    "    N = n\n",
    "    grid_size = grid_size\n",
    "    x = ConnectN(grid_size, N)\n",
    "    \n",
    "    p1 = Q_Learner(x, None, None, player=1)\n",
    "    p2 = Random_Learner(x)\n",
    "    play_game_no_output(x, p1, p2)\n",
    "    \n",
    "    for game in xrange(1, num_trials):\n",
    "                \n",
    "        x = ConnectN(grid_size, N)\n",
    "        p1 = Q_Learner(x, p1.value_table, None, player=1)\n",
    "        p2 = Random_Learner(x)\n",
    "        play_game_no_output(x, p1, p2)\n",
    "\n",
    "        if game == num_trials - 1:\n",
    "            return p1.value_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ConnectN(7,4)\n",
    "x.move(0,1)\n",
    "x.move(1,-1)\n",
    "x.move(1,1)\n",
    "x.move(2,-1)\n",
    "x.move(2,-1)\n",
    "ql = Q_Learner(x, None, True, player=1)\n",
    "ql.calc_next_move(None, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0000000000000000000000000000000000001-100001-1-10000': array([  0.,   0.,  15.,   0.,   0.,   0.,   0.])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ql.value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00000000000000-100000010000001000000-100000011-100-10': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000000000000000000000000000000000-100000010010-10': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000000000000000000000000000000000-100000011-10000': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '0000000000000000000000000000000000000000000000000': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000000000000000000000000000000000000000000010-10': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000000000000000000000000000000000000000001-10000': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000000000000000000000000000000000000000010010-10': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000000000000000000000000000000000000000011-10000': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000000000000000000000000001000000-10000-1010010-10': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000000000000000000000000001000000-100000010010-10': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000000000000000000000000001000000-100000011-100-10': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000000000000000000000000001000000-100000011-10000': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000000000000000000010000001000000-10000-1010010-1-1': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000000000000000000010000001000000-10000-1010010-10': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000000000000000000010000001000000-100000011-100-10': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000000000000100000010000001000000-10000-1-110010-1-1': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000000000000100000010000001000000-10000-1010010-1-1': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000001000000-100000010000001000000-100000011-1-10-10': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000001000000-100000010000001000000-100000011-100-10': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '00000001000000100000010000001000000-10000-1-110010-1-1': array([ 25.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       " '10000001000000-100000010000001000000-10-110-1011-1-10-10': array([  0.,   0., -25.,   0.,   0.,   0.,   0.]),\n",
       " '10000001000000-100000010000001000000-10000-1011-1-10-10': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " '10000001000000-100000010000001000000-100000011-1-10-10': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Q_learner(num_trials=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Q_Learner(TD_Learner):\n",
    "    \"\"\"\n",
    "    Implementation of Q Learning, inheriting from TD Learner base class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task, value_table, known_states, epsilon=.1, discount_factor=.9, learning_rate=.5, player=1, trace_size=.1):\n",
    "        TD_Learner.__init__(self, task, value_table, epsilon, discount_factor, learning_rate, player, trace_size)\n",
    "        self.known_states = known_states\n",
    "\n",
    "    def calc_next_move(self, reward, next_board_state):\n",
    "        if reward is None:\n",
    "            # Approximation of known states. Since too many states, instead, given a board position,\n",
    "            # explore possible moves and give 15 reward to creating streaks of length 3 or 4 and\n",
    "            # 20 reward for preventing an opponent win.\n",
    "            if (self.known_states):\n",
    "                for col in self.task.next_possible_moves():\n",
    "                    row = np.sum([abs(x) for x in next_board_state.grid[col]])\n",
    "                    if next_board_state.streakVertical(next_board_state.grid, col, row - 2, self.player) >= 2:\n",
    "                        self.value_table[grid_to_key(next_board_state.grid)][col] = 15\n",
    "                    temp_board = deepcopy(next_board_state.grid)\n",
    "                    temp_board[col][row] = self.player\n",
    "                    for i in range(0, 4):\n",
    "                        if next_board_state.streakHorizontal(temp_board, col - i, row, self.player) >= 3:\n",
    "                            self.value_table[grid_to_key(next_board_state.grid)][col] = 15\n",
    "                        if next_board_state.streakDiagonalUp(temp_board, col - i, row - i, self.player) >= 3:\n",
    "                            self.value_table[grid_to_key(next_board_state.grid)][col] = 15\n",
    "                        if next_board_state.streakDiagonalDown(temp_board, col - i, row + i, self.player) >= 3:\n",
    "                            self.value_table[grid_to_key(next_board_state.grid)][col] = 15\n",
    "\n",
    "                    if next_board_state.streakVertical(next_board_state.grid, col, row - 3, -self.player) == 3:\n",
    "                        self.value_table[grid_to_key(next_board_state.grid)][col] = 20\n",
    "                    temp_board = deepcopy(next_board_state.grid)\n",
    "                    temp_board[col][row] = -1*self.player\n",
    "                    for i in range(0, 4):\n",
    "                        if next_board_state.streakHorizontal(temp_board, col - i, row, -1*self.player) == 4:\n",
    "                            self.value_table[grid_to_key(next_board_state.grid)][col] = 20\n",
    "                        if next_board_state.streakDiagonalUp(temp_board, col - i, row - i, -1*self.player) == 4:\n",
    "                            self.value_table[grid_to_key(next_board_state.grid)][col] = 20\n",
    "                        if next_board_state.streakDiagonalDown(temp_board, col - i, row + i, -1*self.player) == 4:\n",
    "                            self.value_table[grid_to_key(next_board_state.grid)][col] = 20\n",
    "\n",
    "            next_action = self.softmax(next_board_state)\n",
    "\n",
    "            self.last_board_state = next_board_state.grid\n",
    "\n",
    "            self.last_action = next_action\n",
    "            return self.last_action\n",
    "\n",
    "        if reward == 50:\n",
    "            delta = delta = reward - self.value_table[grid_to_key(self.last_board_state)][self.last_action]\n",
    "            self.value_table[grid_to_key(self.last_board_state)][self.last_action] += self.learning_rate * delta\n",
    "\n",
    "            return self.last_action\n",
    "\n",
    "        if reward == -50:\n",
    "            delta = delta = reward - self.value_table[grid_to_key(self.last_board_state)][self.last_action]\n",
    "            self.value_table[grid_to_key(self.last_board_state)][self.last_action] += self.learning_rate * delta\n",
    "\n",
    "            return self.last_action\n",
    "\n",
    "        \"\"\"\n",
    "        VDBE-Softmax policy. If draw < epsilon, perform Softmax. Else do best action.\n",
    "        \"\"\"\n",
    "        draw = np.random.uniform(0,1,1)\n",
    "\n",
    "        if draw < self.epsilon:\n",
    "            next_action = self.softmax(next_board_state)\n",
    "        else:\n",
    "            next_action = np.argmax(self.value_table[grid_to_key(next_board_state.grid)])\n",
    "\n",
    "        # Update value function.\n",
    "\n",
    "        print(next_board_state.grid)\n",
    "        delta = reward + self.discount_factor * np.amax(self.value_table[grid_to_key(next_board_state.grid)]) - self.value_table[grid_to_key(self.last_board_state)][self.last_action]\n",
    "        self.value_table[grid_to_key(self.last_board_state)][self.last_action] += self.learning_rate * delta\n",
    "\n",
    "        # Update eligibility traces (Watson's Q(lambda))\n",
    "        self.e[grid_to_key(self.last_board_state)][self.last_action] += 1\n",
    "\n",
    "        # Eligibility traces\n",
    "        # Note that here we do not implement classic eligibility traces, which iterate over all state, action pairs\n",
    "        # Instead we consider all next possible board states and update those (for easier computation)\n",
    "        next_possible_moves = next_board_state.next_possible_moves()\n",
    "        next_possible_boards = []\n",
    "\n",
    "        for i in next_possible_moves:\n",
    "            temp_board = deepcopy(next_board_state)\n",
    "            temp_board.move(next_action, self.player)\n",
    "            next_possible_boards.append(temp_board)\n",
    "\n",
    "        for board in next_possible_boards:\n",
    "            valid_actions = board.next_possible_moves()\n",
    "            for action in valid_actions:\n",
    "                self.value_table[grid_to_key(board.grid)][action] += self.learning_rate * delta \\\n",
    "                                                                    * self.e[grid_to_key(board.grid)][action]\n",
    "                if self.last_action == action:\n",
    "                    self.e[grid_to_key(board.grid)][action] = self.discount_factor * self.trace_size \\\n",
    "                                                                    * self.e[grid_to_key(board.grid)][action]\n",
    "                else:\n",
    "                    self.e[grid_to_key(board.grid)][action] = 0\n",
    "\n",
    "        self.last_board_state = next_board_state.grid\n",
    "        self.last_action = next_action\n",
    "\n",
    "        if next_board_state.simulate_move(self.last_action, self.player) == 1:\n",
    "            self.last_action = self.softmax(next_board_state)\n",
    "\n",
    "        return self.last_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 1.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 1.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1. -1. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 1.  1.  1. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1. -1. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 1.  1.  1. -1.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1. -1. -1.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 1.  1.  1. -1.  1.  1.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [-1. -1. -1.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "x = ConnectN(7, 4)\n",
    "p1 = Q_Learner(x, None, None, player=1)\n",
    "p2 = Random_Learner(x)\n",
    "y = play_game_no_output(x, p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input must >= 2-d.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-574eab1bda50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandom_Learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ_Learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game_no_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/AngelaFan/Desktop/Connect-N/helpers.pyc\u001b[0m in \u001b[0;36mplay_game_no_output\u001b[0;34m(board, p1, p2)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mlast_board_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mp2move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_next_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp2move\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-f5030814dadb>\u001b[0m in \u001b[0;36mcalc_next_move\u001b[0;34m(self, reward, next_board_state)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_board_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrid_to_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_board_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrid_to_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_board_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_action\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrid_to_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_board_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_action\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/AngelaFan/Desktop/Connect-N/agents.py\u001b[0m in \u001b[0;36mgrid_to_key\u001b[0;34m(grid)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrot90\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/AngelaFan/anaconda/lib/python2.7/site-packages/numpy-1.9.1-py2.7-macosx-10.5-x86_64.egg/numpy/lib/twodim_base.pyc\u001b[0m in \u001b[0;36mrot90\u001b[0;34m(m, k)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input must >= 2-d.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input must >= 2-d."
     ]
    }
   ],
   "source": [
    "x = ConnectN(7, 4)\n",
    "p1 = Random_Learner(x)\n",
    "p2 = Q_Learner(x, None, None, player=-1)\n",
    "y = play_game_no_output(x, p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
